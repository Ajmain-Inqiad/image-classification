{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AavQ_HHa0RPa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<h3>Natural Image Classification using Pytorch VGGNet and CNN</h3>\n",
        "\n",
        "This is an image classification problem. I have solved it using two method VGGNet and CNN. <a href=\"https://drive.google.com/open?id=19L7j75M9iB4T0YpZ6sboQZ_h-5ryrcVC\">Dataset</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkuUUUkUruTZ",
        "colab_type": "code",
        "outputId": "a81e6b47-a94a-46d8-caf9-dd44941e6a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "\"\"\"Getting the data from google drive\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH0TfaEZPjxW",
        "colab_type": "text"
      },
      "source": [
        "<h2>Image Classification using CNN</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlmmU2wsPo8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Importing Necessary Libraries\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D,BatchNormalization\n",
        "from keras.layers import MaxPooling2D,Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import random\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hOs8aoXP1Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Importing the data folder and giving a shuffle\"\"\"\n",
        "dataset=[]\n",
        "labels=[]\n",
        "random.seed(42)\n",
        "imagePaths = sorted(list(os.listdir(\"/content/drive/My Drive/Colab Notebooks/natural_images\")))\n",
        "random.shuffle(imagePaths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl0tYgXyQD_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for images in imagePaths:\n",
        "    path=sorted(list(os.listdir(\"/content/drive/My Drive/Colab Notebooks/natural_images/\"+images)))\n",
        "    for i in path:\n",
        "        image = cv2.imread(\"/content/drive/My Drive/Colab Notebooks/natural_images/\"+images+'/'+i) #using opencv to read image\n",
        "        image = cv2.resize(image, (128,128)) \n",
        "        image = img_to_array(image) #converting image info to array\n",
        "        dataset.append(image)\n",
        " \n",
        "        l = label = images\n",
        "        labels.append(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05MDV1_oRyIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Converting to numpay array\"\"\"\n",
        "dataset = np.array(dataset, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "\"\"\"Here we are using LabelBinarizer to scale data because it does not need data in integer encoded form first to convert into its respective encoding\"\"\"\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3kJ1yiQTNBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Splitting dataset into train and test\"\"\"\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(dataset,labels,test_size=0.2,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mh2tgafTsBc",
        "colab_type": "text"
      },
      "source": [
        "<h3>Creating CNN model</h3>\n",
        "\n",
        "We will build a sequential classifier. Here we will create full connected CNN model.\n",
        "\n",
        "In first layer as input will come here first, we have patch size 3x3 with input shaper 128 as we have resized image in 128 size. We are using ReLU as an activation function because ReLU is sparsity and reduced likelihood of vanishing gradient. Sigmoid or others might generate some non-zero value resulting in dense representations. In this layer, we are using padding value \"Same\" because tensorflow then tries to spread evenly on both side. To reduce variance, reduce computation complexity and extract low level features from neighbourhood, we need to perform pooling. For that here we have used MaxPooling2D with pool size (2,2) because it extracts the most important features like edges whereas, average pooling extracts features so smoothly. we are using filter size 32 in layer 1. After that, we have used drop out to reduce overfitting an image.\n",
        "\n",
        "For second layer, we have increased the filter size by keeping all other attributes are same. We have done same with other layers.\n",
        "\n",
        "In the last layer we have first flatten the output to get result in array. \n",
        "\n",
        "At the end, we have connected all the layer to make it a fully connected CNN and give output dimension 8 as we have 8 classes. Then we have use softmax as activation function because \n",
        "<ul>\n",
        "  <li>Each value ranges between 0 and 1</li>\n",
        "  <li>The sum of all values is always 1</li>\n",
        "  </ul>\n",
        "  In multi class problem for example 5, output can look like [0, 0, 0, 1, 0] which is easy to understand and compute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdI5HMpATmqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "887b2e22-8a36-4739-a4be-e8ec80dc1ff9"
      },
      "source": [
        "modelClassifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "\n",
        "modelClassifier.add(Convolution2D(32, (3, 3), input_shape = (128, 128, 3), activation = 'relu',padding='same'))\n",
        "\n",
        "modelClassifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "modelClassifier.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "modelClassifier.add(Convolution2D(64, (3, 3), activation = 'relu',padding='same'))\n",
        "\n",
        "modelClassifier.add(Convolution2D(64, (3, 3), activation = 'relu',padding='same'))\n",
        "\n",
        "modelClassifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "modelClassifier.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "modelClassifier.add(Convolution2D(128, (3, 3), activation = 'relu',padding='same'))\n",
        "\n",
        "modelClassifier.add(Convolution2D(128, (3, 3), activation = 'relu',padding='same'))\n",
        "\n",
        "\n",
        "modelClassifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "modelClassifier.add(Dropout(0.25))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "modelClassifier.add(Flatten())\n",
        "modelClassifier.add(Dense(1024,activation='relu'))\n",
        "modelClassifier.add(BatchNormalization())\n",
        "modelClassifier.add(Dropout(0.5))\n",
        "\n",
        "# Step 4 - Full connection\n",
        "modelClassifier.add(Dense(output_dim = 8, activation = 'softmax'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=8)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9CBAGPbYufD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "outputId": "3a23c56d-30e5-4d86-fc95-3337a7468f72"
      },
      "source": [
        "\"\"\"Now we need compile our model\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "For optimizer, we are using Adam optimizer because Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
        "Besides, it is easy to configure.\n",
        "\n",
        "We are using categorical crossentropy as loss function because\n",
        "\n",
        "if we use cross-entropy error, the (output) * (1 – output) term goes away. So, the weight changes don’t get smaller and smaller and so training isn’t s likely to stall out.\n",
        "\"\"\"\n",
        "modelClassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "modelClassifier.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              33555456  \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 8200      \n",
            "=================================================================\n",
            "Total params: 33,845,512\n",
            "Trainable params: 33,843,464\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voWmIjuaZgQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255, #here we need to rescale data as we have set this in our dataset earlier\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                   shear_range=0.2,\n",
        "                                  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpmsC_-1adsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "outputId": "c4d5210e-fa6c-4475-b427-53cd37b860d7"
      },
      "source": [
        "modelClassifier.fit_generator(train_datagen.flow(x_train,y_train,batch_size=512),\n",
        "                         epochs = 20,\n",
        "                         steps_per_epoch=10,\n",
        "                         validation_data=(x_test,y_test),\n",
        "                         )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "10/10 [==============================] - 24s 2s/step - loss: 1.8320 - acc: 0.3012 - val_loss: 13.1551 - val_acc: 0.1812\n",
            "Epoch 2/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 1.5220 - acc: 0.4465 - val_loss: 13.2499 - val_acc: 0.1768\n",
            "Epoch 3/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 1.3786 - acc: 0.5003 - val_loss: 13.1567 - val_acc: 0.1819\n",
            "Epoch 4/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 1.2834 - acc: 0.5360 - val_loss: 13.5213 - val_acc: 0.1609\n",
            "Epoch 5/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 1.2104 - acc: 0.5408 - val_loss: 13.5704 - val_acc: 0.1565\n",
            "Epoch 6/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 1.1215 - acc: 0.5857 - val_loss: 13.5438 - val_acc: 0.1580\n",
            "Epoch 7/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 1.0789 - acc: 0.6047 - val_loss: 13.7588 - val_acc: 0.1464\n",
            "Epoch 8/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 1.0192 - acc: 0.6155 - val_loss: 13.4546 - val_acc: 0.1638\n",
            "Epoch 9/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.9447 - acc: 0.6554 - val_loss: 13.6422 - val_acc: 0.1536\n",
            "Epoch 10/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.8481 - acc: 0.6895 - val_loss: 13.4463 - val_acc: 0.1645\n",
            "Epoch 11/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.8392 - acc: 0.6986 - val_loss: 13.2591 - val_acc: 0.1754\n",
            "Epoch 12/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.8136 - acc: 0.7045 - val_loss: 13.0627 - val_acc: 0.1877\n",
            "Epoch 13/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.7622 - acc: 0.7242 - val_loss: 12.6627 - val_acc: 0.2123\n",
            "Epoch 14/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.6962 - acc: 0.7538 - val_loss: 11.3458 - val_acc: 0.2928\n",
            "Epoch 15/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.7099 - acc: 0.7373 - val_loss: 13.2133 - val_acc: 0.1790\n",
            "Epoch 16/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.6803 - acc: 0.7474 - val_loss: 11.0440 - val_acc: 0.3138\n",
            "Epoch 17/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.6478 - acc: 0.7677 - val_loss: 11.7849 - val_acc: 0.2688\n",
            "Epoch 18/20\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.6287 - acc: 0.7706 - val_loss: 12.3697 - val_acc: 0.2319\n",
            "Epoch 19/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.6161 - acc: 0.7688 - val_loss: 13.4136 - val_acc: 0.1674\n",
            "Epoch 20/20\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.5496 - acc: 0.8034 - val_loss: 13.0663 - val_acc: 0.1891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0ce2e8470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIr_Hui1b3-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d79ea2a-464f-4d42-e481-a1322f090894"
      },
      "source": [
        "\"\"\"Testing whether it can detect image successfully or not\"\"\"\n",
        "\n",
        "list=['airplane','car','cat','dog','flower','fruit','motorbike','person']\n",
        "image = cv2.imread('/content/drive/My Drive/Colab Notebooks/natural_images/flower/flower_0076.jpg')\n",
        "\n",
        "\n",
        "# pre-process the image for classification\n",
        "image = cv2.resize(image, (128,128))\n",
        "image = image.astype(\"float\") / 255.0\n",
        "image = img_to_array(image)\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "\n",
        "pred = modelClassifier.predict(image)[0]\n",
        "\n",
        "for i in range(8):\n",
        "    if pred[i]>0.5:\n",
        "        print(list[i])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flower\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtEeQWqdsgn",
        "colab_type": "text"
      },
      "source": [
        "We can see from above result that, we have sucessfully detected the class of a given image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ7L5CS81sqC",
        "colab_type": "text"
      },
      "source": [
        "<h2>Image Classification using VGGNet</h2>\n",
        "\n",
        "\n",
        "For VGGNet, I have used a pre train model to get higher accuracy.  We train a model on the natural image dataset available on <a href=\"https://drive.google.com/open?id=19L7j75M9iB4T0YpZ6sboQZ_h-5ryrcVC\">this drive link</a> using transfer learning techniques to extract features from a pre-trained model to achieve high accuracy classification of this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2narQTWjrCOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Importing necessary libraries\"\"\"\n",
        "\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torch import optim, cuda\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzIboBlgr53Z",
        "colab_type": "code",
        "outputId": "263653c7-0a1c-4438-e663-ad6eb09bc941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "classes = []\n",
        "image_classes = []\n",
        "n_image = []\n",
        "height = []\n",
        "width = []\n",
        "dimension = []\n",
        "\n",
        "\n",
        "# identifing classes using folder names\n",
        "for folder in os.listdir('/content/drive/My Drive/Colab Notebooks/natural_images'):\n",
        "    classes.append(folder)\n",
        "    \n",
        "    # Getting the number of image\n",
        "    images = os.listdir('/content/drive/My Drive/Colab Notebooks/natural_images/'+folder)\n",
        "    n_image.append(len(images))\n",
        "    for i in images:\n",
        "        image_classes.append(folder)\n",
        "        img = np.array(Image.open('/content/drive/My Drive/Colab Notebooks/natural_images/'+folder+'/'+i))\n",
        "        height.append(img.shape[0])\n",
        "        width.append(img.shape[1])\n",
        "    dimension.append(img.shape[2])\n",
        "    \n",
        "df = pd.DataFrame({\n",
        "    'classes': classes,\n",
        "    'number': n_image,\n",
        "    \"dim\": dimension\n",
        "})\n",
        "print(\"Random heights:\" + str(height[10]), str(height[123]))\n",
        "print(\"Random Widths:\" + str(width[10]), str(width[123]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random heights:110 100\n",
            "Random Widths:271 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo4tXtxxTcZ7",
        "colab_type": "code",
        "outputId": "cc7105a0-a1df-4a35-e95e-8d5a42236a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "image_df = pd.DataFrame({\n",
        "    \"classes\": image_classes,\n",
        "    \"height\": height,\n",
        "    \"width\": width\n",
        "})\n",
        "image_dataframe = image_df.groupby(\"classes\").describe()\n",
        "image_dataframe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">height</th>\n",
              "      <th colspan=\"8\" halign=\"left\">width</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classes</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>airplane</th>\n",
              "      <td>100.0</td>\n",
              "      <td>95.15</td>\n",
              "      <td>19.978461</td>\n",
              "      <td>56.0</td>\n",
              "      <td>80.00</td>\n",
              "      <td>92.0</td>\n",
              "      <td>110.00</td>\n",
              "      <td>169.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>296.60</td>\n",
              "      <td>13.862966</td>\n",
              "      <td>256.0</td>\n",
              "      <td>289.00</td>\n",
              "      <td>297.0</td>\n",
              "      <td>304.00</td>\n",
              "      <td>344.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>car</th>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>100.0</td>\n",
              "      <td>305.57</td>\n",
              "      <td>92.240053</td>\n",
              "      <td>101.0</td>\n",
              "      <td>233.25</td>\n",
              "      <td>311.0</td>\n",
              "      <td>370.25</td>\n",
              "      <td>497.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>308.09</td>\n",
              "      <td>102.849905</td>\n",
              "      <td>117.0</td>\n",
              "      <td>223.50</td>\n",
              "      <td>304.5</td>\n",
              "      <td>399.50</td>\n",
              "      <td>496.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>100.0</td>\n",
              "      <td>304.33</td>\n",
              "      <td>106.701893</td>\n",
              "      <td>50.0</td>\n",
              "      <td>230.75</td>\n",
              "      <td>320.5</td>\n",
              "      <td>372.25</td>\n",
              "      <td>493.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>274.96</td>\n",
              "      <td>112.835262</td>\n",
              "      <td>57.0</td>\n",
              "      <td>187.75</td>\n",
              "      <td>266.0</td>\n",
              "      <td>354.75</td>\n",
              "      <td>493.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flower</th>\n",
              "      <td>100.0</td>\n",
              "      <td>304.65</td>\n",
              "      <td>142.426900</td>\n",
              "      <td>59.0</td>\n",
              "      <td>187.75</td>\n",
              "      <td>316.0</td>\n",
              "      <td>373.50</td>\n",
              "      <td>714.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>363.78</td>\n",
              "      <td>188.117231</td>\n",
              "      <td>70.0</td>\n",
              "      <td>198.50</td>\n",
              "      <td>365.0</td>\n",
              "      <td>485.00</td>\n",
              "      <td>996.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fruit</th>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>motorbike</th>\n",
              "      <td>100.0</td>\n",
              "      <td>108.20</td>\n",
              "      <td>11.737878</td>\n",
              "      <td>71.0</td>\n",
              "      <td>100.75</td>\n",
              "      <td>108.0</td>\n",
              "      <td>116.00</td>\n",
              "      <td>142.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>191.77</td>\n",
              "      <td>10.103360</td>\n",
              "      <td>155.0</td>\n",
              "      <td>187.75</td>\n",
              "      <td>193.0</td>\n",
              "      <td>198.00</td>\n",
              "      <td>213.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>person</th>\n",
              "      <td>100.0</td>\n",
              "      <td>256.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>256.0</td>\n",
              "      <td>256.00</td>\n",
              "      <td>256.0</td>\n",
              "      <td>256.00</td>\n",
              "      <td>256.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>256.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>256.0</td>\n",
              "      <td>256.00</td>\n",
              "      <td>256.0</td>\n",
              "      <td>256.00</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          height                                                           \\\n",
              "           count    mean         std    min     25%    50%     75%    max   \n",
              "classes                                                                     \n",
              "airplane   100.0   95.15   19.978461   56.0   80.00   92.0  110.00  169.0   \n",
              "car        100.0  100.00    0.000000  100.0  100.00  100.0  100.00  100.0   \n",
              "cat        100.0  305.57   92.240053  101.0  233.25  311.0  370.25  497.0   \n",
              "dog        100.0  304.33  106.701893   50.0  230.75  320.5  372.25  493.0   \n",
              "flower     100.0  304.65  142.426900   59.0  187.75  316.0  373.50  714.0   \n",
              "fruit      100.0  100.00    0.000000  100.0  100.00  100.0  100.00  100.0   \n",
              "motorbike  100.0  108.20   11.737878   71.0  100.75  108.0  116.00  142.0   \n",
              "person     100.0  256.00    0.000000  256.0  256.00  256.0  256.00  256.0   \n",
              "\n",
              "           width                                                           \n",
              "           count    mean         std    min     25%    50%     75%    max  \n",
              "classes                                                                    \n",
              "airplane   100.0  296.60   13.862966  256.0  289.00  297.0  304.00  344.0  \n",
              "car        100.0  100.00    0.000000  100.0  100.00  100.0  100.00  100.0  \n",
              "cat        100.0  308.09  102.849905  117.0  223.50  304.5  399.50  496.0  \n",
              "dog        100.0  274.96  112.835262   57.0  187.75  266.0  354.75  493.0  \n",
              "flower     100.0  363.78  188.117231   70.0  198.50  365.0  485.00  996.0  \n",
              "fruit      100.0  100.00    0.000000  100.0  100.00  100.0  100.00  100.0  \n",
              "motorbike  100.0  191.77   10.103360  155.0  187.75  193.0  198.00  213.0  \n",
              "person     100.0  256.00    0.000000  256.0  256.00  256.0  256.00  256.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL6vKNaK4G4F",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the output that the dataset is well balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFQhsLTJTk57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation and normalization for training and testing\n",
        "# normalization for validation\n",
        "# transform.compose clubs all the transforms provided to it.\n",
        "\n",
        "transform_images = {\n",
        "    # Train data using data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.95, 1.0)), #This will extract a patch of size (256, 224) from input image and scale down to (.95,1)\n",
        "        transforms.RandomRotation(degrees=15), #rotating image to an angle to get better view\n",
        "        transforms.ColorJitter(), #giving default colorjitter so that it does not change  brightness, contrast and saturation of image.\n",
        "        transforms.RandomHorizontalFlip(), #using default to check what is probability of an image being flipped (default 0.5)\n",
        "        transforms.CenterCrop(size=224),  # Cropping the center part of image from 256 size. Image net standards\n",
        "        transforms.ToTensor(), #converting input image to PyTorch tensor\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # scaling input data. Imagenet standards\n",
        "    ]),\n",
        "    # Validation data using data augmentation\n",
        "    'val':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Test data using data augmentation\n",
        "    'test':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxdgTDv_TqHq",
        "colab_type": "code",
        "outputId": "d4427574-a842-49f5-d82d-808c3c1a162e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "batch_size_list = [128, 256, 512] #setting up many batch size to use them for letter\n",
        "\n",
        "batch = batch_size_list[0] #selecting a batch size\n",
        "\n",
        "#getting all the data\n",
        "full_data = datasets.ImageFolder(root='/content/drive/My Drive/Colab Notebooks/natural_images')\n",
        "\n",
        "# getting length of train, test and validation dataset length\n",
        "train_data_len = int(len(full_data)*0.8) #getting 80% of dataset as train\n",
        "valid_data_len = int((len(full_data) - train_data_len)/2) #getting half of rest data as validation set\n",
        "test_data_len = int(len(full_data) - train_data_len - valid_data_len) #getting rest of dataset as test data\n",
        "\n",
        "# splitting dataset into train, test and validation set\n",
        "train_data, val_data, test_data = random_split(full_data, [train_data_len, valid_data_len, test_data_len])\n",
        "train_data.dataset.transform = transform_images['train']\n",
        "val_data.dataset.transform = transform_images['val']\n",
        "test_data.dataset.transform = transform_images['test']\n",
        "\n",
        "print(\"Length of train dataset: {} \\nLength of validation dataset: {} \\nLength of test dataset: {}\".format(len(train_data), len(val_data), len(test_data)))\n",
        "\n",
        "#loading dataset using dataloader from pytorch\n",
        "train_loader = DataLoader(train_data, batch_size=batch, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch, shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train dataset: 5519 \n",
            "Length of validation dataset: 690 \n",
            "Length of test dataset: 690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDV9vmd9T2bk",
        "colab_type": "code",
        "outputId": "ad6c476c-432b-48a5-d9d8-f87b2b147809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#getting training data\n",
        "train_iterator = iter(train_loader)\n",
        "features, labels = next(train_iterator)\n",
        "print(features.shape, labels.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 224, 224]) torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlwmFVEaT59b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "fd28aeaf-f247-41a9-c62b-4d5fcd6f0a3d"
      },
      "source": [
        "\"\"\"Getting pre trained VGGNet Model\"\"\"\n",
        "model = models.vgg16(pretrained=True)\n",
        "model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Dropout(p=0.5)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace)\n",
              "    (5): Dropout(p=0.5)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO-Htd8K_PCU",
        "colab_type": "text"
      },
      "source": [
        "As we can see that, this model uses Conv2d with kernel size (3,3), stride (1,1) and padding (1,1). It uses ReLU inplace and MaxPool2d with kernel size 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsQ_d-a2UIoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to freeze early layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2onZU6OAHbt",
        "colab_type": "text"
      },
      "source": [
        "After freezing the pre-trained layers of the network, we need to define classifier layer which we will train to suit our dataset and use case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GT026kEUNN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Model classifier with dropout\"\"\"\n",
        "\n",
        "no_classes = 8\n",
        "no_inputs = model.classifier[6].in_features\n",
        "# we are going to use 6th classifier\n",
        "# in which case input_features=4096, output_features=1000\n",
        "\n",
        "# Add on classifier\n",
        "model.classifier[6] = nn.Sequential(\n",
        "    nn.Linear(no_inputs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4), #handling dropout\n",
        "    nn.Linear(256, no_classes),\n",
        "    nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY91BFMbURin",
        "colab_type": "code",
        "outputId": "bf1b8456-7b2e-4ce1-fc65-00130e21a6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "source": [
        "\"\"\"\n",
        "Here for loss function we are using Negative Logarithmic Likelihood\n",
        "Because it is used when the model outputs a probability for each class, rather than just the most likely class. It is a “soft” measurement of accuracy that incorporates the idea of probabilistic confidence.\n",
        "\"\"\"\n",
        "scale = nn.NLLLoss()\n",
        "\n",
        "\"\"\"\n",
        "For optimizer, we are using Adam optimizer because Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
        "Besides, it is easy to configure\n",
        "\"\"\"\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Dropout(p=0.5)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace)\n",
              "    (5): Dropout(p=0.5)\n",
              "    (6): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.4)\n",
              "      (3): Linear(in_features=256, out_features=8, bias=True)\n",
              "      (4): LogSoftmax()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9O0ansKUYFP",
        "colab_type": "code",
        "outputId": "6db5e645-85e2-4182-f51c-0bdb85738872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "\"\"\"Converting class to integer number so that it is easy to execute\"\"\"\n",
        "model.class_to_idx = full_data.class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "print(\"List of class index: \")\n",
        "list(model.idx_to_class.items())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of class index: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'airplane'),\n",
              " (1, 'car'),\n",
              " (2, 'cat'),\n",
              " (3, 'dog'),\n",
              " (4, 'flower'),\n",
              " (5, 'fruit'),\n",
              " (6, 'motorbike'),\n",
              " (7, 'person')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3MzpFNBUb25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,criterion,optimizer,train_loader,val_loader,save_location,early_stop=3,n_epochs=20,print_every=2):\n",
        "   \n",
        "#Initializing some variables\n",
        "  valid_loss_min = np.Inf\n",
        "  stop_count = 0\n",
        "  valid_max_acc = 0\n",
        "  history = []\n",
        "  model.epochs = 0\n",
        "  \n",
        "  #Loop starts here\n",
        "  for epoch in range(n_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    \n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    ii = 0\n",
        "    \n",
        "    for data, label in train_loader:\n",
        "      ii += 1\n",
        "      data, label = data.cuda(), label.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      \n",
        "      loss = criterion(output, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Track train loss by multiplying average loss by number of examples in batch\n",
        "      train_loss += loss.item() * data.size(0)\n",
        "      \n",
        "      # Calculate accuracy by finding max log probability\n",
        "      _, pred = torch.max(output, dim=1) # first output gives the max value in the row(not what we want), second output gives index of the highest val\n",
        "      correct_tensor = pred.eq(label.data.view_as(pred)) # using the index of the predicted outcome above, torch.eq() will check prediction index against label index to see if prediction is correct(returns 1 if correct, 0 if not)\n",
        "      accuracy = torch.mean(correct_tensor.type(torch.FloatTensor)) #tensor must be float to calc average\n",
        "      train_acc += accuracy.item() * data.size(0)\n",
        "      if ii%15 == 0:\n",
        "        print(f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete.')\n",
        "      \n",
        "    model.epochs += 1\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      \n",
        "      for data, label in val_loader:\n",
        "        data, label = data.cuda(), label.cuda()\n",
        "        \n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "        \n",
        "        _, pred = torch.max(output, dim=1)\n",
        "        correct_tensor = pred.eq(label.data.view_as(pred))\n",
        "        accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "        valid_acc += accuracy.item() * data.size(0)\n",
        "        \n",
        "      train_loss = train_loss / len(train_loader.dataset)\n",
        "      valid_loss = valid_loss / len(val_loader.dataset)\n",
        "      \n",
        "      train_acc = train_acc / len(train_loader.dataset)\n",
        "      valid_acc = valid_acc / len(val_loader.dataset)\n",
        "      \n",
        "      history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "      \n",
        "      if (epoch + 1) % print_every == 0:\n",
        "        print(f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}')\n",
        "        print(f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%')\n",
        "        \n",
        "      if valid_loss < valid_loss_min:\n",
        "        torch.save(model.state_dict(), save_location)\n",
        "        stop_count = 0\n",
        "        valid_loss_min = valid_loss\n",
        "        valid_best_acc = valid_acc\n",
        "        best_epoch = epoch\n",
        "        \n",
        "      else:\n",
        "        stop_count += 1\n",
        "        \n",
        "        # Below is the case where we handle the early stop case\n",
        "        if stop_count >= early_stop:\n",
        "          print(f'\\nEarly Stopping Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%')\n",
        "          model.load_state_dict(torch.load(save_location))\n",
        "          model.optimizer = optimizer\n",
        "          history = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_acc','valid_acc'])\n",
        "          return model, history\n",
        "        \n",
        "  model.optimizer = optimizer\n",
        "  print(f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%')\n",
        "  \n",
        "  history = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if__jhTDU2A0",
        "colab_type": "code",
        "outputId": "9319daf2-5464-47a1-8e35-1e1d96e1bd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "model, history = train(model, scale, optimizer, train_loader, val_loader, save_location='./natural_images_using_vggnet.pt', early_stop=6, n_epochs=30, print_every=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\t36.36% complete.\n",
            "Epoch: 0\t70.45% complete.\n",
            "Epoch: 1\t36.36% complete.\n",
            "Epoch: 1\t70.45% complete.\n",
            "\n",
            "Epoch: 1 \tTraining Loss: 0.0215 \tValidation Loss: 0.0500\n",
            "\t\tTraining Accuracy: 99.53%\t Validation Accuracy: 99.28%\n",
            "\n",
            "Best epoch: 1 with loss: 0.05 and acc: 99.28%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFQnBHE5MVEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "4b518865-a9ab-4f30-ad70-19b673ba1147"
      },
      "source": [
        "history"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>valid_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.145062</td>\n",
              "      <td>0.05383</td>\n",
              "      <td>0.964486</td>\n",
              "      <td>0.991304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.021486</td>\n",
              "      <td>0.04999</td>\n",
              "      <td>0.995289</td>\n",
              "      <td>0.992754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_loss  valid_loss  train_acc  valid_acc\n",
              "0    0.145062     0.05383   0.964486   0.991304\n",
              "1    0.021486     0.04999   0.995289   0.992754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rkXZ0GCNMVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Getting accuracy on test data\"\"\"\n",
        "def accuracy(model, test_loader, loss):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    for data, label in test_loader:\n",
        "      data, label = data.cuda(), label.cuda()\n",
        "      \n",
        "      output = model(data)\n",
        "      \n",
        "      _, pred = torch.max(output, dim=1)\n",
        "      correct_tensor = pred.eq(label.data.view_as(pred))\n",
        "      accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "      test_acc += accuracy.item() * data.size(0)\n",
        "      \n",
        "    test_acc = test_acc / len(test_loader.dataset)\n",
        "    return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1GiLGkvNRfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0cefabb-63cf-4bee-8de9-ede6c522db5c"
      },
      "source": [
        "model.load_state_dict(torch.load('./natural_images.pt'))\n",
        "test_acc = accuracy(model.cuda(), test_loader, scale)\n",
        "print(f'The model has achieved an accuracy of {100 * test_acc:.2f}% on the test dataset')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has achieved an accuracy of 99.71% on the test dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrm6yeiMeFrr",
        "colab_type": "text"
      },
      "source": [
        "<h2>Discussion</h2>\n",
        "\n",
        "For this problem, we can that\n",
        "<table>\n",
        "  <thead>\n",
        "    <th>Spec</th>\n",
        "    <th>CNN with cv2</th>\n",
        "    <th>VGGNet</th>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Validation loss</td>\n",
        "      <td>0.1891</td>\n",
        "      <td>0.04999</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Test score</td>\n",
        "      <td> 0.8034</td>\n",
        "      <td>0.9971</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "  </table>\n",
        "  \n",
        "  VGGNet is performing better than CNN with CV2 model. Because VGGNet  is characterized by its simplicity, using only 3×3 convolutional layers stacked on top of each other in increasing depth. Reducing volume size is handled by max pooling.\n",
        "  Unfortunately, there are two major drawbacks with VGGNet:\n",
        "  <ul>\n",
        "  <li>It is slow to train.</li>\n",
        "  <li>The network architecture weights themselves are quite large</li>\n",
        "</ul>\n",
        "\n",
        "There are some difficulties I have faced during this problem. In google colab, to run VGGNet sometimes, runtime memory becomes full and i had to manage session variables.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pntxfj0tgrsu",
        "colab_type": "text"
      },
      "source": [
        "<h2>Reference</h2>\n",
        "\n",
        "1. https://keras.io/preprocessing/image/\n",
        "2. https://www.quora.com/What-are-the-advantages-of-the-Adam-and-RMSProp-optimization-algorithms-over-gradient-descent-or-stochastic-gradient-descent\n",
        "3. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "4. https://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling\n",
        "5. https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
        "6. https://keras.io/layers/convolutional/\n",
        "7. https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/\n",
        "8. https://www.quora.com/What-is-the-benefit-of-using-softmax-function-in-the-last-layer-of-DNN-What-is-the-relation-between-cross-entropy-and-loss-functions\n",
        "9. https://medium.com/themlblog/image-data-augmentation-using-keras-a6a61edbc59f\n",
        "10. https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/"
      ]
    }
  ]
}
